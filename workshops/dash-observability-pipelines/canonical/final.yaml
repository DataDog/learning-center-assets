enterprise:
  api_key: "${DD_API_KEY}"
  configuration_key: "${OP_CONFIGURATION_KEY}"
  site: "datadoghq.com"
api:
  enabled: true
  address: 0.0.0.0:8686
  playground: false

sources:
  datadog_agent:
    address: 0.0.0.0:8282
    type: datadog_agent
    multiple_outputs: true

transforms:
  parse_logs:
    type: remap
    inputs:
      - datadog_agent.logs
    source: |-
      # This additional unwrapping is needed because of the journald source of the logs.
      # We are explicit about error handling here so we can make sure that we leave non-HTTP
      # messages alone.
      msg, err = parse_json(.message)
      if err == null {
        . |= object!(msg)

        msg, err = parse_apache_log(.message, format:"common")
        if err == null {
          .http = msg
        }
      }
  
  # Pull out AWS VPC flow logs
  parse_vpc_logs:
    type: remap
    inputs:
      - parse_logs
    source: |-
      msg, err = parse_aws_vpc_flow_log(.message)
      if err == null {
        .vpc = msg
      }

  pass_logs:
    type: route
    inputs:
      - parse_vpc_logs
    route:
      http: "exists(.http)"
      vpc: "exists(.vpc)"

  # HTTP lane
  sample_useless_http_logs:
    type: sample
    inputs:
      - pass_logs.http
    exclude: ".http.status > 200 ?? true"
    rate: 100

  set_http_status:
    type: remap
    inputs:
      - sample_useless_http_logs
    source: |-
      if exists(.http) {
        s, err = (.http.status > 200)
        if err == null && s {
          .status = "error"
        }
      }

  http_log_metrics:
    type: log_to_metric
    inputs:
      - set_http_status
    metrics:
      - type: counter
        field: status
        name: http.response_code
        namespace: storedog
        tags:
          status: "{{http.status}}"
          path: "{{http.path}}"

  cardinality_control:
    type: tag_cardinality_limit
    inputs:
      - http_log_metrics
    limit_exceeded_action: drop_tag
    mode: probabilistic
    value_limit: 100

  # VPC Flow Logs.
  filter_vpc_logs:
    type: route
    inputs:
      - pass_logs.vpc
    route:
      #accept: '(.vpc.action == "ACCEPT")'
      reject: '(.vpc.action == "REJECT")'

  vpc_log_metrics:
    type: log_to_metric
    inputs:
      - pass_logs.vpc
    metrics:
      - type: counter
        field: status
        name: network.connections
        namespace: storedog
        tags:
          status: "{{vpc.action}}"

  # Everything else.
  remove_cc:
    type: remap
    inputs:
      - pass_logs._unmatched
    source: |-
      msg, err = redact(.message, filters:[r'\d{13, 16}'])
      if err == null {
        .message = msg
        .redacted = true
      }

  redacted:
    type: route
    inputs:
      - remove_cc
    route:
      yes: "exists(.redacted)"

  redacted_metrics:
    type: log_to_metric
    inputs:
      - redacted.yes
    metrics:
      - type: counter
        field: status
        name: redacted
        namespace: security

  tag_logs:
    type: remap
    inputs:
      - set_http_status
      - remove_cc
    source: |-
      # Parse the received `.ddtags` field so we can more easily access the contained tags.
      .ddtags = parse_key_value!(.ddtags, key_value_delimiter: ":", field_delimiter: ",")

      # Add some tags so that we can positively identify that data has passed through OP.
      .ddtags.sender = "observability_pipelines"
      .ddtags.op_aggregator = get_hostname!()

      # Re-encode Datadog tags in the format the intakes expect.
      .ddtags = encode_key_value(.ddtags, key_value_delimiter: ":", field_delimiter: ",")

  tag_metrics:
    type: remap
    inputs:
      - cardinality_control
      - redacted_metrics
      - datadog_agent.metrics
    source: |-
      # Add some tags so that we can positively identify that data has passed through OP.
      .tags.sender = "observability_pipelines"
      .tags.op_aggregator = get_hostname!()

sinks:
  datadog_logs:
    type: datadog_logs
    inputs:
      - tag_logs
    default_api_key: ${DD_API_KEY}
    compression: gzip
  datadog_metrics:
    type: datadog_metrics
    inputs:
      - tag_metrics
    default_api_key: ${DD_API_KEY}
  minio:
    type: aws_s3
    inputs:
      - datadog_agent.logs
    auth:
      access_key_id: minioadmin
      secret_access_key: minioadmin
    bucket: lab
    region: us-east-1
    endpoint: "http://localhost:9000"
    encoding:
      codec: json