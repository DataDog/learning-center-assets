enterprise:
  api_key: "${DD_API_KEY}"
  configuration_key: "${OP_CONFIGURATION_KEY}"
  site: "datadoghq.com"
api:
  enabled: true
  address: 0.0.0.0:8686
  playground: false

sources:
  datadog_agent:
    address: 0.0.0.0:8282
    type: datadog_agent
    multiple_outputs: true

transforms:
  # This step exists because the logs originate from services started by systemd,
  # and the Datadog Agent adds this metadata when it reads from journald.
  # We recommend using this as your input for downstream processing steps, so
  # you can avoid having to deal with the extra overhead.
  remove_journald:
    type: remap
    inputs:
      - datadog_agent.logs
    source: |-
      msg, err = parse_json(.message)
      if err == null {
        . |= object!(msg)
        del(.journald)
      }

# transforms:
  # This step attempts to parse the messages as Apache Common Format
  # logs, and adds whatever it is able to parse as metadata to the
  # log.
  #
  # We are explicit about error handling here so we can make sure
  # that we leave non-HTTP messages alone, or so we can add
  # additional formats that we want to try easily onto the end.
  parse_logs:
    type: remap
    inputs:
      - remove_journald
    source: |-
      msg, err = parse_apache_log(.message, format:"common")
      if err == null {
        .http = msg
      }

# transforms:
  # This will allow us to differentiate our downstream steps depending
  # on what parsing succeeded previously, since we don't want non-HTTP
  # logs to go through the same filtering and other steps that we're
  # going to be setting up.
  route_log_type:
    type: route
    inputs:
      - parse_logs
    route:
      http: "exists(.http)"

# transforms:
  # This step can replace the existing deal_with_useless_http_logs.
  # Anything that would have failed the condition previously is
  # now sampled aggressively before being forwarded.
  #
  # This configuration will allow 1% of 200 OK logs through.
  deal_with_useless_http_logs:
    type: sample
    inputs:
      - route_log_type.http
    exclude: ".http.status > 200 ?? true"
    rate: 100

# transforms:
  # This will show errors properly categorized in the Log Explorer,
  # instead of always showing up as `INFO`
  set_http_status:
    type: remap
    inputs:
      - deal_with_useless_http_logs
    source: |-
      if exists(.http) {
        s, err = (.http.status > 200)
        if err == null && s {
          .status = "error"
        }
      }

# transforms:
  http_log_metrics:
    type: log_to_metric
    inputs:
      - route_log_type.http
    metrics:
      - type: counter
        field: status
        name: http.response_code
        namespace: storedog
        tags:
          status: "{{http.status}}"
          path: "{{http.path}}"

# transforms:
  cardinality_control:
    type: tag_cardinality_limit
    inputs:
      - http_log_metrics
    limit_exceeded_action: drop_tag
    mode: probabilistic
    value_limit: 100

# transforms:
  remove_cc:
    type: remap
    inputs:
      - route_log_type._unmatched
    source: |-
      msg, err = redact(.message, filters:[r'\d{13, 16}'])
      if err == null {
        .message = msg
        .redacted = true
      }

# transforms:
  redacted:
    type: route
    inputs:
      - remove_cc
    route:
      yes: "exists(.redacted)"

  redacted_metrics:
    type: log_to_metric
    inputs:
      - redacted.yes
    metrics:
      - type: counter
        field: status
        name: redacted
        namespace: security

# transforms:
  tag_logs:
    type: remap
    inputs:
      - set_http_status
      - remove_cc
    source: |-
      # Parse the received `.ddtags` field so we can more easily access the contained tags.
      .ddtags = parse_key_value!(.ddtags, key_value_delimiter: ":", field_delimiter: ",")

      # Add some tags so that we can positively identify that data has passed through OP.
      .ddtags.sender = "observability_pipelines"
      .ddtags.op_aggregator = get_hostname!()

      # Re-encode Datadog tags in the format the intakes expect.
      .ddtags = encode_key_value(.ddtags, key_value_delimiter: ":", field_delimiter: ",")

# transforms:
  tag_metrics:
    type: remap
    inputs:
      - cardinality_control
      - datadog_agent.metrics
      - redacted_metrics
    source: |-
      # Add some tags so that we can positively identify that data has passed through OP.
      .tags.sender = "observability_pipelines"
      .tags.op_aggregator = get_hostname!()

sinks:
  datadog_logs:
    type: datadog_logs
    inputs:
      - tag_logs
    default_api_key: ${DD_API_KEY}
    compression: gzip
  datadog_metrics:
    type: datadog_metrics
    inputs:
      - tag_metrics
    default_api_key: ${DD_API_KEY}
# sinks:
  minio:
    type: aws_s3
    inputs:
      - datadog_agent.logs
    auth:
      access_key_id: minioadmin
      secret_access_key: minioadmin
    bucket: lab
    region: us-east-1
    endpoint: "http://localhost:9000"
    encoding:
      codec: json
